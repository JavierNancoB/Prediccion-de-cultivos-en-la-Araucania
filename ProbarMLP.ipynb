{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c30a3-097b-4877-97a2-cdd7d5694201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo MLP con 7 experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f973aa-0ad8-49da-b5c6-66ac66e49308",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grupo: 1\n",
    "\n",
    "Estudiantes:\n",
    "\n",
    "-Constanza Olivos Fernandez\n",
    "\n",
    "-Javier Nanco Becerra\n",
    "\n",
    "-Nicol√°s Pozo Villagr√°n\n",
    "\n",
    "Fecha: 20-09-2025\n",
    "\n",
    "Version: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9cdac-28bc-4d08-995b-543c44920dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivos del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd9bc7-0c5e-4e09-9be0-da512026aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizaci√≥n de Hiperpar√°metros para Modelo MLP en Predicci√≥n de Rendimiento de Trigo\n",
    "\n",
    "## Objetivo del Estudio\n",
    "**Identificar la combinaci√≥n √≥ptima de hiperpar√°metros** (batch size y learning rate) para un modelo de Perceptr√≥n Multicapa (MLP) mediante la ejecuci√≥n sistem√°tica de 7 experimentos controlados, con el fin de maximizar la precisi√≥n predictiva del rendimiento de cultivos de trigo.\n",
    "\n",
    "## Objetivos Espec√≠ficos\n",
    "\n",
    "### 1. Evaluaci√≥n Comparativa de Configuraciones\n",
    "- Analizar el impacto de **5 valores de batch size** (16, 32, 64, 128, 256) en el desempe√±o del modelo\n",
    "- Evaluar el efecto de **4 tasas de aprendizaje** (0.0001, 0.001, 0.002, 0.005, 0.01) en la convergencia y estabilidad\n",
    "- Determinar la **combinaci√≥n sin√©rgica** que produce el mejor resultado predictivo\n",
    "\n",
    "### 2. An√°lisis de Comportamiento del Modelo\n",
    "- Medir la **capacidad de generalizaci√≥n** mediante m√©tricas de rendimiento (R¬≤, RMSE, MAE)\n",
    "- Identificar patrones de **convergencia y estabilidad** durante el entrenamiento\n",
    "- Detectar configuraciones propensas a **sobreajuste o subajuste**\n",
    "\n",
    "### 3. Establecimiento de L√≠nea Base\n",
    "- Crear un **benchmark de desempe√±o** para futuras optimizaciones\n",
    "- Documentar el **proceso experimental** para reproducibilidad\n",
    "- Generar **recomendaciones pr√°cticas** para configuraciones de MLP en problemas similares\n",
    "\n",
    "## Hip√≥tesis de Trabajo\n",
    "\"Una configuraci√≥n de **batch size moderado (32-64)** combinada con un **learning rate relativamente alto (0.01)** producir√° el mejor equilibrio entre velocidad de convergencia y capacidad predictiva final en modelos MLP para datos agron√≥micos.\"\n",
    "\n",
    "## M√©trica Principal de Evaluaci√≥n\n",
    "- **Variable principal**: Coeficiente de determinaci√≥n (R¬≤)\n",
    "- **Variables secundarias**: RMSE, MAE, P√©rdida final de entrenamiento\n",
    "- **Criterio de selecci√≥n**: Maximizaci√≥n de R¬≤ en conjunto de prueba\n",
    "\n",
    "## Alcance del An√°lisis\n",
    "El presente estudio se centra exclusivamente en la **optimizaci√≥n hiperparam√©trica b√°sica**, sentando las bases para investigaciones futuras que podr√≠an incluir:\n",
    "- B√∫squeda m√°s exhaustiva de hiperpar√°metros\n",
    "- Optimizaci√≥n de arquitectura de red\n",
    "- T√©cnicas avanzadas de regularizaci√≥n\n",
    "- Validaci√≥n en m√∫ltiples conjuntos de datos\n",
    "\n",
    "## Valor Agregado\n",
    "Este experimento proporciona **evidencia emp√≠rica** sobre la sensibilidad de los modelos MLP a configuraciones hiperparam√©tricas espec√≠ficas en el dominio agron√≥mico, facilitando decisiones informadas en futuros desarrollos de modelos predictivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0019ea86-f68e-4337-ac2a-0075008bcdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cargado\n",
      "üöÄ Iniciando 7 experimentos...\n",
      "üß™ Ejecutando Exp1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\javie\\AppData\\Local\\Temp\\ipykernel_41672\\2800762466.py:117: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  experimentos_df = pd.concat([experimentos_df, pd.DataFrame([nuevo_exp])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exp1 completado - R¬≤: 0.7977, Loss: 336329.09\n",
      "üß™ Ejecutando Exp2...\n",
      "‚úÖ Exp2 completado - R¬≤: 0.8205, Loss: 236732.55\n",
      "üß™ Ejecutando Exp3...\n",
      "‚úÖ Exp3 completado - R¬≤: 0.7111, Loss: 390126.56\n",
      "üß™ Ejecutando Exp4...\n",
      "‚úÖ Exp4 completado - R¬≤: 0.7445, Loss: 376843.03\n",
      "üß™ Ejecutando Exp5...\n",
      "‚úÖ Exp5 completado - R¬≤: 0.7976, Loss: 361023.66\n",
      "üß™ Ejecutando Exp6...\n",
      "‚úÖ Exp6 completado - R¬≤: 0.7930, Loss: 366013.44\n",
      "üß™ Ejecutando Exp7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exp7 completado - R¬≤: 0.7946, Loss: 326125.44\n",
      "\n",
      "==========================================================================================\n",
      "üìä REGISTRO DE 7 EXPERIMENTOS - MLP\n",
      "==========================================================================================\n",
      "Experimento           Configuraci√≥n                               Resultado                                 Observaciones\n",
      "       Exp1  batch = 32, lr = 0.001  R¬≤: 79.8%; Loss: 336329.1; RMSE: 332.9      Configuraci√≥n base, convergencia estable\n",
      "       Exp2   batch = 32, lr = 0.01 R¬≤: 82.0%; Loss: 236732.6; RMSE: 313.54                LR alto, posible inestabilidad\n",
      "       Exp3 batch = 32, lr = 0.0001 R¬≤: 71.1%; Loss: 390126.6; RMSE: 397.81                   LR bajo, convergencia lenta\n",
      "       Exp4 batch = 128, lr = 0.001 R¬≤: 74.4%; Loss: 376843.0; RMSE: 374.07        Batch grande, entrenamiento m√°s r√°pido\n",
      "       Exp5  batch = 16, lr = 0.001 R¬≤: 79.8%; Loss: 361023.7; RMSE: 332.99          Batch peque√±o, m√°s updates por √©poca\n",
      "       Exp6  batch = 64, lr = 0.002 R¬≤: 79.3%; Loss: 366013.4; RMSE: 336.72                        Combinaci√≥n balanceada\n",
      "       Exp7 batch = 256, lr = 0.005 R¬≤: 79.5%; Loss: 326125.4; RMSE: 335.44 Configuraci√≥n agresiva, riesgo de divergencia\n",
      "\n",
      "üíæ Registro de 7 experimentos guardado en 'csv/registro_experimentos_mlp.csv'\n",
      "üèÜ MEJOR EXPERIMENTO: Exp2\n",
      "   Configuraci√≥n: batch = 32, lr = 0.01\n",
      "   R¬≤: 0.8205\n",
      "   RMSE: 313.54\n",
      "   MAE: 247.51\n",
      "   √âpocas: 90\n",
      "   Observaciones: LR alto, posible inestabilidad\n",
      "\n",
      "üìà AN√ÅLISIS COMPARATIVO:\n",
      "   Rango de R¬≤: 0.7111 - 0.8205\n",
      "   Mejor batch size: 32\n",
      "   Mejor learning rate: 0.01\n",
      "   Diferencia entre mejor/peor R¬≤: 0.1094\n",
      "üíæ Resumen de m√©tricas guardado en 'csv/resumen_experimentos_mlp.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Entrenamiento del modelo MLP con 7 experimentos\n",
    "# ===========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar TensorFlow para output m√≠nimo\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# ===========================\n",
    "# 1. Cargar y preparar datos\n",
    "# ===========================\n",
    "csv_path = os.path.join('csv', 'features_trigo.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\" Dataset cargado\")\n",
    "\n",
    "# One-hot encoding\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Features y target\n",
    "X = df_encoded.drop(\"Rendimiento_kg_ha\", axis=1)\n",
    "y = df_encoded[\"Rendimiento_kg_ha\"]\n",
    "\n",
    "# Split 70/15/15\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===========================\n",
    "# 2. Crear DataFrame para registro de experimentos\n",
    "# ===========================\n",
    "experimentos_df = pd.DataFrame(columns=[\n",
    "    'Experimento', 'Fecha', 'Batch_Size', 'Learning_Rate', 'Epochs_Final',\n",
    "    'R2', 'RMSE', 'MAE', 'Loss_Final', 'Loss_Val_Final', 'Observaciones'\n",
    "])\n",
    "\n",
    "# ===========================\n",
    "# 3. Funci√≥n para crear modelo MLP con learning rate configurable\n",
    "# ===========================\n",
    "def crear_mlp(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# 4. Funci√≥n para ejecutar y registrar experimentos\n",
    "# ===========================\n",
    "def ejecutar_experimento(nombre, batch_size, learning_rate, observaciones=\"\"):\n",
    "    print(f\" Ejecutando {nombre}...\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    model = crear_mlp(learning_rate=learning_rate)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_data=(X_valid_scaled, y_valid),\n",
    "        epochs=100,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    epochs_final = len(history.history['loss'])\n",
    "    \n",
    "    # Registrar experimento\n",
    "    nuevo_exp = {\n",
    "        'Experimento': nombre,\n",
    "        'Fecha': datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
    "        'Batch_Size': batch_size,\n",
    "        'Learning_Rate': learning_rate,\n",
    "        'Epochs_Final': epochs_final,\n",
    "        'R2': round(r2, 4),\n",
    "        'RMSE': round(rmse, 2),\n",
    "        'MAE': round(mae, 2),\n",
    "        'Loss_Final': round(final_loss, 2),\n",
    "        'Loss_Val_Final': round(final_val_loss, 2),\n",
    "        'Observaciones': observaciones\n",
    "    }\n",
    "    \n",
    "    global experimentos_df\n",
    "    experimentos_df = pd.concat([experimentos_df, pd.DataFrame([nuevo_exp])], ignore_index=True)\n",
    "    \n",
    "    print(f\"‚úÖ {nombre} completado - R¬≤: {r2:.4f}, Loss: {final_loss:.2f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# ===========================\n",
    "# 5. Ejecutar 7 experimentos diferentes\n",
    "# ===========================\n",
    "print(\" Iniciando 7 experimentos...\")\n",
    "\n",
    "# Experimento 1: Configuraci√≥n base\n",
    "model1, history1 = ejecutar_experimento(\n",
    "    nombre=\"Exp1\",\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    observaciones=\"Configuraci√≥n base, convergencia estable\"\n",
    ")\n",
    "\n",
    "# Experimento 2: Learning rate m√°s alto\n",
    "model2, history2 = ejecutar_experimento(\n",
    "    nombre=\"Exp2\", \n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    observaciones=\"LR alto, posible inestabilidad\"\n",
    ")\n",
    "\n",
    "# Experimento 3: Learning rate m√°s bajo\n",
    "model3, history3 = ejecutar_experimento(\n",
    "    nombre=\"Exp3\",\n",
    "    batch_size=32, \n",
    "    learning_rate=0.0001,\n",
    "    observaciones=\"LR bajo, convergencia lenta\"\n",
    ")\n",
    "\n",
    "# Experimento 4: Batch size m√°s grande\n",
    "model4, history4 = ejecutar_experimento(\n",
    "    nombre=\"Exp4\",\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    observaciones=\"Batch grande, entrenamiento m√°s r√°pido\"\n",
    ")\n",
    "\n",
    "# Experimento 5: Batch size m√°s peque√±o\n",
    "model5, history5 = ejecutar_experimento(\n",
    "    nombre=\"Exp5\",\n",
    "    batch_size=16,\n",
    "    learning_rate=0.001,\n",
    "    observaciones=\"Batch peque√±o, m√°s updates por √©poca\"\n",
    ")\n",
    "\n",
    "# Experimento 6: Combinaci√≥n √≥ptima\n",
    "model6, history6 = ejecutar_experimento(\n",
    "    nombre=\"Exp6\",\n",
    "    batch_size=64,\n",
    "    learning_rate=0.002,\n",
    "    observaciones=\"Combinaci√≥n balanceada\"\n",
    ")\n",
    "\n",
    "# Experimento 7: Configuraci√≥n agresiva\n",
    "model7, history7 = ejecutar_experimento(\n",
    "    nombre=\"Exp7\",\n",
    "    batch_size=256,\n",
    "    learning_rate=0.005,\n",
    "    observaciones=\"Configuraci√≥n agresiva, riesgo de divergencia\"\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# 6. Mostrar y guardar resultados\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"REGISTRO DE 7 EXPERIMENTOS - MLP\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Mostrar tabla formateada\n",
    "resultados_display = experimentos_df.copy()\n",
    "resultados_display['Configuraci√≥n'] = 'batch = ' + experimentos_df['Batch_Size'].astype(str) + ', lr = ' + experimentos_df['Learning_Rate'].astype(str)\n",
    "resultados_display['Resultado'] = 'R¬≤: ' + (experimentos_df['R2'] * 100).round(1).astype(str) + '%; Loss: ' + experimentos_df['Loss_Final'].round(1).astype(str) + '; RMSE: ' + experimentos_df['RMSE'].astype(str)\n",
    "\n",
    "tabla_final = resultados_display[['Experimento', 'Configuraci√≥n', 'Resultado', 'Observaciones']]\n",
    "print(tabla_final.to_string(index=False))\n",
    "\n",
    "# Guardar registro completo\n",
    "experimentos_df.to_csv(\"csv/registro_experimentos_mlp.csv\", index=False)\n",
    "print(f\"\\nRegistro de 7 experimentos guardado en 'csv/registro_experimentos_mlp.csv'\")\n",
    "\n",
    "# ===========================\n",
    "# 7. Identificar y guardar el mejor modelo\n",
    "# ===========================\n",
    "mejor_exp = experimentos_df.loc[experimentos_df['R2'].idxmax()]\n",
    "mejores_modelos = [model1, model2, model3, model4, model5, model6, model7]\n",
    "mejor_modelo = mejores_modelos[experimentos_df['R2'].idxmax()]\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "mejor_modelo.save(\"modelos/MLP_mejor_modelo.h5\")\n",
    "print(f\"MEJOR EXPERIMENTO: {mejor_exp['Experimento']}\")\n",
    "print(f\"   Configuraci√≥n: batch = {mejor_exp['Batch_Size']}, lr = {mejor_exp['Learning_Rate']}\")\n",
    "print(f\"   R¬≤: {mejor_exp['R2']:.4f}\")\n",
    "print(f\"   RMSE: {mejor_exp['RMSE']:.2f}\")\n",
    "print(f\"   MAE: {mejor_exp['MAE']:.2f}\")\n",
    "print(f\"   √âpocas: {mejor_exp['Epochs_Final']}\")\n",
    "print(f\"   Observaciones: {mejor_exp['Observaciones']}\")\n",
    "\n",
    "# ===========================\n",
    "# 8. An√°lisis comparativo\n",
    "# ===========================\n",
    "print(\"\\nAN√ÅLISIS COMPARATIVO:\")\n",
    "print(f\"   Rango de R¬≤: {experimentos_df['R2'].min():.4f} - {experimentos_df['R2'].max():.4f}\")\n",
    "print(f\"   Mejor batch size: {mejor_exp['Batch_Size']}\")\n",
    "print(f\"   Mejor learning rate: {mejor_exp['Learning_Rate']}\")\n",
    "print(f\"   Diferencia entre mejor/peor R¬≤: {(experimentos_df['R2'].max() - experimentos_df['R2'].min()):.4f}\")\n",
    "\n",
    "# Guardar m√©tricas resumidas\n",
    "resumen_metricas = {\n",
    "    'Total_Experimentos': len(experimentos_df),\n",
    "    'Mejor_R2': experimentos_df['R2'].max(),\n",
    "    'Peor_R2': experimentos_df['R2'].min(),\n",
    "    'R2_Promedio': experimentos_df['R2'].mean(),\n",
    "    'Mejor_Configuracion': f\"batch={mejor_exp['Batch_Size']}, lr={mejor_exp['Learning_Rate']}\",\n",
    "    'Mejor_Experimento': mejor_exp['Experimento']\n",
    "}\n",
    "\n",
    "pd.DataFrame([resumen_metricas]).to_csv(\"csv/resumen_experimentos_mlp.csv\", index=False)\n",
    "print(\"Resumen de m√©tricas guardado en 'csv/resumen_experimentos_mlp.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f7064-6d8b-4365-af0a-24822fe25273",
   "metadata": {},
   "source": [
    "# An√°lisis de Experimentos MLP\n",
    "\n",
    "## 1. Resultados Destacados\n",
    "- **Mejor experimento: Exp2**\n",
    "  - Configuraci√≥n: `batch = 32, lr = 0.01`\n",
    "  - R¬≤ = **0.8205**\n",
    "  - RMSE = **313.54**\n",
    "  - MAE = **247.51**\n",
    "  - Observaci√≥n: un learning rate m√°s alto permiti√≥ un mejor aprendizaje, aunque con riesgo de inestabilidad.\n",
    "\n",
    "- **Peor experimento: Exp3**\n",
    "  - Configuraci√≥n: `batch = 32, lr = 0.0001`\n",
    "  - R¬≤ = **0.7111**\n",
    "  - RMSE = **397.81**\n",
    "  - Observaci√≥n: el learning rate demasiado bajo provoc√≥ convergencia lenta y bajo desempe√±o.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Impacto del Batch Size\n",
    "- **Batch peque√±o (16)** ‚Üí buen desempe√±o (R¬≤ ‚âà 0.798), pero entrenamiento m√°s lento por mayor n√∫mero de actualizaciones.  \n",
    "- **Batch grande (256)** ‚Üí desempe√±o aceptable (R¬≤ ‚âà 0.795), pero con riesgo de saltarse m√≠nimos locales.  \n",
    "- **Batch intermedio (32 y 64)** ‚Üí ofrecen un buen equilibrio entre estabilidad y velocidad de entrenamiento.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tendencias Identificadas\n",
    "- El **learning rate** tiene mayor influencia que el batch size.  \n",
    "  - Muy bajo ‚Üí convergencia lenta y peores m√©tricas.  \n",
    "  - Muy alto ‚Üí puede dar el mejor resultado, pero con riesgo de inestabilidad.  \n",
    "- El **batch size** afecta m√°s la **estabilidad y velocidad** que la m√©trica final.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Diferencia entre Mejor y Peor\n",
    "- Mejor R¬≤: **0.8205 (Exp2)**  \n",
    "- Peor R¬≤: **0.7111 (Exp3)**  \n",
    "- Brecha de **0.1094 (~11%)** en capacidad de explicaci√≥n ‚Üí diferencia significativa en el rendimiento del modelo.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusi√≥n\n",
    "- **Exp2 (batch = 32, lr = 0.01)** es la mejor configuraci√≥n, aunque requiere vigilancia por posible inestabilidad.  \n",
    "- **Exp1 (batch = 32, lr = 0.001)** es m√°s estable, con R¬≤ solo ligeramente menor (0.798 vs 0.8205).  \n",
    "- Para futuros experimentos se recomienda:  \n",
    "  - Probar **learning rates intermedios** (0.005, 0.007).  \n",
    "  - Usar **early stopping** para mejorar la convergencia de modelos con LR bajos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
